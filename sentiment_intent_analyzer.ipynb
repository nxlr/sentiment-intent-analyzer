{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_intent_analyzer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMapNqGfAhkjNv7L89jzepS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxlr/sentiment-intent-analyzer/blob/main/sentiment_intent_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7fbRdPb_vTW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Packages\n",
        "!pip install transformers\n",
        "!pip install git+https://github.com/tweepy/tweepy.git\n",
        "!pip install tensorflow\n",
        "!pip install streamlit\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAMxOcjNv37T",
        "outputId": "49c4780a-6cf5-49c8-c3d7-4b67fa0fae40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/tweepy/tweepy.git\n",
            "  Cloning https://github.com/tweepy/tweepy.git to /tmp/pip-req-build-d2c9mfa_\n",
            "  Running command git clone -q https://github.com/tweepy/tweepy.git /tmp/pip-req-build-d2c9mfa_\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.0) (3.2.0)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.0) (2.27.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.10.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.0) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.0) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.0) (2022.5.18.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.1)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (1.9.1)\n",
            "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.0.1)\n",
            "Requirement already satisfied: click<8.1,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.8)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.11.3)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.3.5)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.19.0)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.0)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.21.6)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.1.27)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.13.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.4.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->streamlit) (3.8.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2022.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12->streamlit) (1.15.0)\n",
            "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (6.13.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.1.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.7.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.5.5)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.3)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (7.3.1)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (7.33.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.4.8)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.0.29)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.4.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.6.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.1.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.10.0)\n",
            "Requirement already satisfied: pyzmq>=22.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (23.0.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.15.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.0.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "import tweepy\n",
        "import streamlit as st\n",
        "import sentencepiece as spm\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "Waq_6Eri_8ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set API keys and Authentication method\n",
        "api_key = 'ixHfMyHeS4kZoNrTzY4VWxwmV'\n",
        "api_key_secret = '5iSOFztWXx5pc2nlzxzTrzwuZSEMXRf8l2sggosuS8VNo8rTeL'\n",
        "access_token = '788065986502987777-aNqg4hWd3AqNGSo6H26D0pbzCFkMyfN'\n",
        "access_token_secret = '4vtX80WsjFqPEyj25T6gqXZ0BJsLkpBOGLxcHLJuIuz0o'\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ],
      "metadata": {
        "id": "91fhzcis_9Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove pretrained model if it already exists.\n",
        "!rm -r cardiffnlp/\n",
        "# Test the Roberta Sentiment Analysis model\n",
        "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n",
        "sentiment_task(\"Johnny Depp\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-aKPw_qADpx",
        "outputId": "405adaed-34b0-4cce-cddd-e6aec1890158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "rm: cannot remove 'cardiffnlp/': No such file or directory\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'Neutral', 'score': 0.6338866353034973}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SENTIMENT AND INTENT ANALYSIS\n",
        "!rm -r cardiffnlp/\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "# Preprocess text (username and link placeholders)\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "\n",
        "# PT\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "model.save_pretrained(MODEL)\n",
        "\n",
        "# Test Sentiment Analysis Model on sample text below\n",
        "text = \"crap shit worse bad pollution negative horse cat\"\n",
        "text = preprocess(text)\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "scores = output[0][0].detach().numpy()\n",
        "scores = softmax(scores)\n",
        "\n",
        "# # TF\n",
        "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "# model.save_pretrained(MODEL)\n",
        "\n",
        "# text = \"Good night üòä\"\n",
        "# encoded_input = tokenizer(text, return_tensors='tf')\n",
        "# output = model(encoded_input)\n",
        "# scores = output[0][0].numpy()\n",
        "# scores = softmax(scores)\n",
        "\n",
        "# Print labels and scores\n",
        "ranking = np.argsort(scores)\n",
        "ranking = ranking[::-1]\n",
        "for i in range(scores.shape[0]):\n",
        "    l = config.id2label[ranking[i]]\n",
        "    s = scores[ranking[i]]\n",
        "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIq05nzwAAkk",
        "outputId": "5c486a2a-ec6d-450a-d351-4ed269389f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "1) Negative 0.9479\n",
            "2) Neutral 0.0417\n",
            "3) Positive 0.0104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Script to Extract tweets of a\n",
        "# particular Hashtag using Tweepy and Pandas\n",
        "\n",
        "!rm -r cardiffnlp/\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoModelWithLMHead\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "# Preprocess text (username and link placeholders)\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
        "config1 = AutoConfig.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
        "\n",
        "# Sentiment Analysis Model\n",
        "model1 = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
        "model1.save_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
        "\n",
        "# Intent Analysis Model\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
        "model2 = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
        "\n",
        "def get_intent(event, max_length=16):\n",
        "  input_text = \"%s </s>\" % event\n",
        "  features = tokenizer2([input_text], return_tensors='pt')\n",
        "\n",
        "  output = model2.generate(input_ids=features['input_ids'], \n",
        "               attention_mask=features['attention_mask'],\n",
        "               max_length=max_length)\n",
        "\n",
        "  return tokenizer2.decode(output[0])\n",
        "\n",
        "# function to display data of each tweet\n",
        "def printtweetdata(n, ith_tweet):\n",
        "    print()\n",
        "    print(f\"Tweet {n}:\")\n",
        "    print(f\"Username:{ith_tweet[0]}\")\n",
        "    print(f\"Description:{ith_tweet[1]}\")\n",
        "    print(f\"Location:{ith_tweet[2]}\")\n",
        "    print(f\"Following Count:{ith_tweet[3]}\")\n",
        "    print(f\"Follower Count:{ith_tweet[4]}\")\n",
        "    print(f\"Total Tweets:{ith_tweet[5]}\")\n",
        "    print(f\"Retweet Count:{ith_tweet[6]}\")\n",
        "    print(f\"Tweet Text:{ith_tweet[7]}\")\n",
        "    print(f\"Hashtags Used:{ith_tweet[8]}\")\n",
        "    print(f\"Positive:{ith_tweet[9]}\")\n",
        "    print(f\"Neutral:{ith_tweet[10]}\")\n",
        "    print(f\"Negative:{ith_tweet[11]}\")\n",
        "    print(f\"Intention:{ith_tweet[12]}\")\n",
        "\n",
        "\n",
        "# function to perform data extraction\n",
        "def scrape(words, date_since, numtweet):\n",
        "\n",
        "    # Creating DataFrame using pandas\n",
        "    db = pd.DataFrame(columns=['username',\n",
        "                'description',\n",
        "                'location',\n",
        "                'following',\n",
        "                'followers',\n",
        "                'totaltweets',\n",
        "                'retweetcount',\n",
        "                'text',\n",
        "                'hashtags',\n",
        "                'positive',\n",
        "                'neutral',\n",
        "                'negative',\n",
        "                'intention'])\n",
        "\n",
        "    # We are using .Cursor() to search\n",
        "    # through twitter for the required tweets.\n",
        "    # The number of tweets can be\n",
        "    # restricted using .items(number of tweets)\n",
        "    tweets = tweepy.Cursor(api.search_tweets,\n",
        "              words, lang=\"en\",\n",
        "              since_id=date_since,\n",
        "              tweet_mode='extended').items(numtweet)\n",
        "\n",
        "\n",
        "    # .Cursor() returns an iterable object. Each item in\n",
        "    # the iterator has various attributes\n",
        "    # that you can access to\n",
        "    # get information about each tweet\n",
        "    list_tweets = [tweet for tweet in tweets]\n",
        "\n",
        "    # Counter to maintain Tweet Count\n",
        "    i = 1\n",
        "\n",
        "    # we will iterate over each tweet in the\n",
        "    # list for extracting information about each tweet\n",
        "    for tweet in list_tweets:\n",
        "        username = tweet.user.screen_name\n",
        "        description = tweet.user.description\n",
        "        location = tweet.user.location\n",
        "        following = tweet.user.friends_count\n",
        "        followers = tweet.user.followers_count\n",
        "        totaltweets = tweet.user.statuses_count\n",
        "        retweetcount = tweet.retweet_count\n",
        "        hashtags = tweet.entities['hashtags']\n",
        "\n",
        "        # Retweets can be distinguished by\n",
        "        # a retweeted_status attribute,\n",
        "        # in case it is an invalid reference,\n",
        "        # except block will be executed\n",
        "        try:\n",
        "            text = tweet.retweeted_status.full_text\n",
        "        except AttributeError:\n",
        "            text = tweet.full_text\n",
        "        hashtext = list()\n",
        "        for j in range(0, len(hashtags)):\n",
        "            hashtext.append(hashtags[j]['text'])\n",
        "      \n",
        "        # Sentiment Analysis\n",
        "        preprocessed_text = preprocess(text)\n",
        "        encoded_input = tokenizer1(preprocessed_text, return_tensors='pt')\n",
        "        output = model1(**encoded_input)\n",
        "        scores = output[0][0].detach().numpy()\n",
        "        scores = softmax(scores)\n",
        "\n",
        "        # Print labels and scores\n",
        "        ranking = np.argsort(scores)\n",
        "        ranking = ranking[::-1]\n",
        "        for j in range(scores.shape[0]):\n",
        "            l = config.id2label[ranking[j]]\n",
        "            s = scores[ranking[j]]\n",
        "\n",
        "            if l == \"Positive\":\n",
        "              positive = np.round(float(s), 4)\n",
        "            elif l == \"Neutral\":\n",
        "              neutral = np.round(float(s), 4)\n",
        "            else:\n",
        "              negative = np.round(float(s), 4)\n",
        "\n",
        "        event = text\n",
        "        intention = get_intent(event)  \n",
        "\n",
        "        # Here we are appending all the\n",
        "        # extracted information in the DataFrame\n",
        "        ith_tweet = [username, description,\n",
        "              location, following,\n",
        "              followers, totaltweets,\n",
        "              retweetcount, text, hashtext, positive, neutral, negative, intention]\n",
        "        db.loc[len(db)] = ith_tweet\n",
        "\n",
        "        # Function call to print tweet data on screen\n",
        "        printtweetdata(i, ith_tweet)\n",
        "        i = i + 1\n",
        "    \n",
        "    # we will save our database as a CSV file.\n",
        "    filename = 'scraped_tweets.csv'\n",
        "    db.to_csv(filename)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\t\t# Enter Text or Hastag below and initial date\n",
        "\t\twords = \"crisis\"\n",
        "\t\t# yyyy-mm-dd\n",
        "\t\tdate_since = \"2022-04-01\"\n",
        "\n",
        "\t\t# number of tweets you want to extract in one run\n",
        "\t\tnumtweet = 5\n",
        "\t\tscrape(words, date_since, numtweet)\n",
        "\t\tprint('Scraping has completed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Gyjkmj3lCi",
        "outputId": "007c6b9b-8c8a-4eec-d6d4-0d8134186a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet 1:\n",
            "Username:shaun_vids\n",
            "Description:he/him.  \n",
            "https://t.co/CXFt8dAqbV \n",
            "https://t.co/Z2XYWcMC95\n",
            "Location:\n",
            "Following Count:629\n",
            "Follower Count:293614\n",
            "Total Tweets:22644\n",
            "Retweet Count:0\n",
            "Tweet Text:in the neutral fiction world, a crisis is a crisis, interrupting the norm. in our world, crisis is an inherent, and necessary, result of capitalism. we're always building to the next bust. \"crisis coming\" reads \"business as usual\" to the ones causing the crisis\n",
            "Hashtags Used:[]\n",
            "Positive:0.0365\n",
            "Neutral:0.1738\n",
            "Negative:0.7898\n",
            "Intention:<pad> to be a failure</s>\n",
            "\n",
            "Tweet 2:\n",
            "Username:K_Brown1996\n",
            "Description:25üë∏üèæ\n",
            "Location:Cocoa, FL‚òÄÔ∏è\n",
            "Following Count:242\n",
            "Follower Count:503\n",
            "Total Tweets:80478\n",
            "Retweet Count:37393\n",
            "Tweet Text:‚ÄúMillennials aren‚Äôt having children‚Äù\n",
            "\n",
            "The world: \n",
            "School shootings\n",
            "Baby formula shortage \n",
            "Inflation \n",
            "$7.25 minimum wage\n",
            "Housing crisis\n",
            "Poor health care \n",
            "High mortality rate \n",
            "Student loan debt\n",
            "War in Europe \n",
            "White supremacy \n",
            "Women‚Äôs rights in jeopardy\n",
            "Hashtags Used:[]\n",
            "Positive:0.0131\n",
            "Neutral:0.1286\n",
            "Negative:0.8583\n",
            "Intention:<pad> to be a parent</s>\n",
            "\n",
            "Tweet 3:\n",
            "Username:jojox9581\n",
            "Description:Girl they got me ü§ßüò≠\n",
            "Location:Houston, TX\n",
            "Following Count:1312\n",
            "Follower Count:1027\n",
            "Total Tweets:14520\n",
            "Retweet Count:37393\n",
            "Tweet Text:‚ÄúMillennials aren‚Äôt having children‚Äù\n",
            "\n",
            "The world: \n",
            "School shootings\n",
            "Baby formula shortage \n",
            "Inflation \n",
            "$7.25 minimum wage\n",
            "Housing crisis\n",
            "Poor health care \n",
            "High mortality rate \n",
            "Student loan debt\n",
            "War in Europe \n",
            "White supremacy \n",
            "Women‚Äôs rights in jeopardy\n",
            "Hashtags Used:[]\n",
            "Positive:0.0131\n",
            "Neutral:0.1286\n",
            "Negative:0.8583\n",
            "Intention:<pad> to be a parent</s>\n",
            "\n",
            "Tweet 4:\n",
            "Username:adelaidehendo\n",
            "Description:Flesh & Bone she/her\n",
            "Location:Atlanta, GA ‚òÜ‚òÜ‚òÜ‚òÜ\n",
            "Following Count:220\n",
            "Follower Count:76\n",
            "Total Tweets:3606\n",
            "Retweet Count:37393\n",
            "Tweet Text:‚ÄúMillennials aren‚Äôt having children‚Äù\n",
            "\n",
            "The world: \n",
            "School shootings\n",
            "Baby formula shortage \n",
            "Inflation \n",
            "$7.25 minimum wage\n",
            "Housing crisis\n",
            "Poor health care \n",
            "High mortality rate \n",
            "Student loan debt\n",
            "War in Europe \n",
            "White supremacy \n",
            "Women‚Äôs rights in jeopardy\n",
            "Hashtags Used:[]\n",
            "Positive:0.0131\n",
            "Neutral:0.1286\n",
            "Negative:0.8583\n",
            "Intention:<pad> to be a parent</s>\n",
            "\n",
            "Tweet 5:\n",
            "Username:TrueInd17894991\n",
            "Description:India FirstüáÆüá≥, Akahnda Bharat, Jai hind, Hindu Rastra,\n",
            "Location:\n",
            "Following Count:52\n",
            "Follower Count:112\n",
            "Total Tweets:10274\n",
            "Retweet Count:460\n",
            "Tweet Text:Pakistan Govt hikes petrol price by ‚Çπ30 to resume IMF program.\n",
            "\n",
            "We are blessed to have Modiji as PM who reduced fuel rates despite world crisis.\n",
            "Hashtags Used:[]\n",
            "Positive:0.5971\n",
            "Neutral:0.2286\n",
            "Negative:0.1743\n",
            "Intention:<pad> to be a good leader</s>\n",
            "Scraping has completed!\n"
          ]
        }
      ]
    }
  ]
}